L’objectif de cette première section est de donner une vue d’ensemble du processus de modélisation tout en précisant certains détails relatifs au prétraitement des données. Nous passerons ensuite à l’exposition des résultats et à l’interprétation du modèle sélectionné à l’issue de la phase d’entraînement. À des fins de concision, l’’ensemble des éléments est présenté sous forme de remarques très condensées. 

1. **Le caractère itératif du processus de modélisation** : les sources consultées pour élaborer une méthode de modélisation mettaient l’accent sur le processus itératif, presque tâtonnant du processus.  Toutefois, le chemin que nous avons suivi en pratique n’a pas nécessité de multiples itérations en raison du niveau de performance obtenus par les meilleurs modèles dès le premier entraînement. 
1. **La réduction de la dimension et la sélection de caractéristiques** : la crainte de voir le “fléau de la dimension” affecter drastiquement les performances de nos modèles, crainte que nous avions en tête à l’issue de l’analyse exploratoire, ne s’est pas révélée fondée. Aussi, il ne nous a pas été nécessaire de recourir à l’expertise métier ni d'utiliser des techniques spécifiques de sélection de variables (le *SelectKbest*, *SelectFromModel*, le *RFECV*, etc) que nous avions envisagées *a priori*. 
1. **Automatiser autant que possible** : afin de pouvoir condenser notre codes, de gagner du temps en cas d’itération multiples, et de faciliter la réutilisation des codes dans le cadre d’autres projets, le choix a été fait d’utiliser des fonctions, des pipelines (pour les modèles requérant une standardisation, et des boucles. Nous avons ainsi pu gagner un temps certain pour le preprocessing, l’évaluation des modèles, le calcul des résidus et des métriques. 
1. **Double modélisation** : notre problème consistait à voir s’il était possible de construire un modèle plus complexe, plus riche, plus nuancé permettant d’expliquer les déterminants de la satisfaction dans la vie en recourant à des variables distinctes du niveau de richesse par habitant. Afin de pouvoir évaluer la réussite de notre entreprise, il nous a paru intéressant d’opérer une double modélisation. Dans un premier temps, nous avons donc entraîné un modèle unvariée, utilisant comme variable le niveau de richesse par habitant mis sur une échelle logarithmique.  
1. **Gestion des valeurs manquantes** : nous avons essayé deux stratégies. La première consista à éliminer les données manquantes et à vérifier l’incidence de la modification sur  la distribution de notre variable cible. Le jeu de données fut ainsi réduit à une cinquantaine d’observations, et l’allure de la distribution en fut modifiée. Ce qui nous décida à remplacer les valeurs manquantes par la moyenne de chaque variable. 
1. **Normalisation des données** : ici, lorsque le modèle l’exigeait, nous avons opté pour une standardisation avec l’algorithme *StandardScaler()* de Sickit-learn. L’usage d’un *RobustScaler*() qui s’appuie sur la médiane des séries a été un temps envisagé, mais le peu de valeurs extrêmes présentes dans le jeu de données nous a paru justifier le recours à une standardisation courante. 
1. **Jeu d’entraînement, jeu de test et généralisation** : initialement, nous avions envisagé une segmentation du jeu de données en essayant de stratifier les échantillons en fonction du niveau de richesse et/ou de satisfaction dans la vie. Toutefois, après échange avec notre tuteur, nous avons décidé d’entraîner nos modèles sur l’ensemble du jeu de données. Deux raisons ont justifié ce choix :  1) la modeste taille du jeu de données et 2) le fait que la question de la *généralisation* à d’autres pays ne se pose pas vraiment dans notre contexte, compte tenu des données disponibles. 
1. **La question de la validation croisée** : la petitesse de jeu de données, eu égard à la question de la validation croisée, s’est révélée un avantage. En effet, il nous a été possible de recourir à l’utilisation de la stratégie*LeaveOneOut()* afin de pouvoir obtenir les conditions de validations les plus robustes du point de vue statistiques, sans rendre l’entraînement trop long. 
1. **Les 8 modèles** : en combinant une recherche livresque, la *mind map* disponible sur le site de Sickit-learn, et le graphique mettant en relation le degré de complexité et d’interprétabilité des modèles (Les deux représentations sont intégrées ci-dessous), une première liste a pu être élaborée, comprenant les six modèles suivants : *Ridge, Lasso, ElasticNet, Knn, DecisionTree, SVR*. Étant donné la nature de problème, qui exige de pouvoir nourrir les décisions en mati!ère de politiques publique, une préférence avait initialement été accordée à des modèles interprétables. Ce qui nous a conduit à exclure les méthodes d’ensemble, et à n’admettre comme modèle le plus complexe qu’un algorithme de SVM. Toutefois, notre tuteur nous a conseillé d’ajouter deux algorithmes plus complexes appartenant à la classe des méthodes ensemblistes et fondés sur la construction d’arbre de décision : un *Random Forest* et un *XGBoost*. Et afin de pouvoir rendre les prédictions intelligibles, il nous a recommandé d’utiliser la bibliothèque Shap. 
1. **L’évaluation par grille et les hyperparamètres** : L’étape suivante a consisté à élaborer un dictionnaire d'hyper paramètres pour chaque modèle. Pour ce faire, les notebooks, ainsi que différents ouvrages ou articles consultés en lignes ont permis de construire un premier dictionnaire présentant plusieurs milliers de combinaisons à tester. Ayant pu obtenir une bonne performance avec ce premier dictionnaire, nous n’avons pas jugé utile d’optimiser les hyperparamètres via une nouvelle itération. Par ailleurs, le temps d’entraînement nous a également incité à renoncer. L’entraînement a pris environ dix heures. 
1. **Les métriques** : pour l’évaluation des modèles, cinq métriques ont été produites afin d’examiner les performances de nos modèles sous plusieurs angles. L’entraînement des modèles avec *GridSearchCV* a été réglé pour retourner le *coefficient de détermination de Pearson* ou *r2* qui nous informe sur la capacité du modèle à embrasser la variance des données. À cette première métrique, furent ajoutées la *Rmse*, la *M(ean)se*, la *M(edian)ae* ainsi que *max\_error*, toutes disponibles dans Sickit-learn. À cela nous avons ajouté une représentation graphique des résidus pour les deux meilleurs algorithmes. La variété de métriques utilisées s’explique largement par le fait que nous n’avions pas d’*a priori* profondément ancré sur la façon de pondérer les erreurs de prédiction. 
1. **L’interprétation des résultats** : Après avoir procédé à l’entraînement des modèles, puis à l’analyse comparative avec les 5 métriques, nous avons retenu un modèle que nous nous sommes efforcés de comprendre en utilisant différents graphiques de la bibliothèque Shap. 


